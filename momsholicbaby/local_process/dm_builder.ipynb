{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA9LaWlXnsBa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhgq5nBhnuBq",
        "outputId": "bf5b0797-207c-4c4a-9f96-e52f1e47cbd1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kiwipiepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZTiAfktn7lL",
        "outputId": "0b3ff00e-7afe-4df2-c832-043943e945f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kiwipiepy\n",
            "  Downloading kiwipiepy-0.20.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting kiwipiepy_model<0.21,>=0.20 (from kiwipiepy)\n",
            "  Downloading kiwipiepy_model-0.20.0.tar.gz (34.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from kiwipiepy) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kiwipiepy) (4.67.1)\n",
            "Downloading kiwipiepy-0.20.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kiwipiepy_model\n",
            "  Building wheel for kiwipiepy_model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kiwipiepy_model: filename=kiwipiepy_model-0.20.0-py3-none-any.whl size=34818026 sha256=ac6216685989972d6773e67ef91b8d8e510bd97e1f1feae5e4008da73b32599e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/c8/52/3a539d6e9065b191fe1c215e0203dcc3e00601c0e3d3d39824\n",
            "Successfully built kiwipiepy_model\n",
            "Installing collected packages: kiwipiepy_model, kiwipiepy\n",
            "Successfully installed kiwipiepy-0.20.4 kiwipiepy_model-0.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from kiwipiepy import Kiwi\n",
        "import os\n",
        "import json\n",
        "from io import BytesIO\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq"
      ],
      "metadata": {
        "id": "TdgsTJmMnt_D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_for_local(input_parquet_path, output_path, stopwords_path=None):\n",
        "    \"\"\"\n",
        "    로컬 Parquet 파일을 처리하여 로컬 디렉토리에 저장\n",
        "\n",
        "    Parameters:\n",
        "    - input_parquet_path: 입력 Parquet 파일 경로\n",
        "    - output_path: 결과 파일을 저장할 로컬 디렉토리 경로\n",
        "    - stopwords_path: 불용어 파일 경로 (선택사항)\n",
        "    \"\"\"\n",
        "    # 1. 데이터 로드\n",
        "    print(\"데이터 로드 중...\")\n",
        "    table = pq.read_table(input_parquet_path)\n",
        "    df = table.to_pandas()\n",
        "\n",
        "    # 2. 불용어 로드 (제공된 경우)\n",
        "    FILTER_WORDS = []\n",
        "    if stopwords_path and os.path.exists(stopwords_path):\n",
        "        with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "            FILTER_WORDS = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    # 3. 날짜 변환 (raw_date 형식 'YYYY.MM.DD. HH:MM')\n",
        "    print(\"날짜 변환 중...\")\n",
        "    df['post_date'] = pd.to_datetime(df['raw_date'].str.split('.').str[:3].str.join('-'), format='%Y-%m-%d', errors='coerce')\n",
        "    df['post_month'] = df['post_date'].dt.month\n",
        "    df['post_year'] = df['post_date'].dt.year\n",
        "\n",
        "    # 4. 토큰 추출\n",
        "    print(\"토큰 추출 중...\")\n",
        "    TERM_POSES = [\"NNP\", \"NNG\", \"XR\"]\n",
        "    kiwi = Kiwi()\n",
        "\n",
        "    # 각 텍스트에서 토큰과 품사 정보 추출\n",
        "    def extract_tokens_with_pos(text):\n",
        "        analyzed = kiwi.analyze(text)\n",
        "        tokens_with_pos = []\n",
        "        for sent in analyzed:\n",
        "            for word_info in sent[0]:\n",
        "                token, pos = word_info[0], word_info[1]\n",
        "                is_stopword = token in FILTER_WORDS\n",
        "                if pos in TERM_POSES:\n",
        "                    tokens_with_pos.append({\n",
        "                        'token': token,\n",
        "                        'pos': pos,\n",
        "                        'is_stopword': is_stopword\n",
        "                    })\n",
        "        return tokens_with_pos\n",
        "\n",
        "    # 토큰 정보 추출\n",
        "    print(\"각 게시글에서 토큰 추출 중...\")\n",
        "    df['tokens_with_pos'] = df['question_text'].apply(extract_tokens_with_pos)\n",
        "\n",
        "    # 5. 키워드 그룹 정의 및 추출\n",
        "    print(\"키워드 그룹 정보 추출 중...\")\n",
        "    keyword_groups = {\n",
        "        'hospital': ['병원', '소아과'],\n",
        "        'symptom': ['증상'],\n",
        "        'skin': ['피부'],\n",
        "        'fever': ['열'],\n",
        "        'cough': ['기침'],\n",
        "        'runny_nose': ['콧물'],\n",
        "        'stool': ['변', '똥'],\n",
        "        'hand_foot': ['수족'],\n",
        "        'diarrhea': ['설사'],\n",
        "        'allergy': ['알러지', '알레르기'],\n",
        "        'atopy': ['아토피'],\n",
        "        'urticaria': ['두드러기'],\n",
        "        'vomit': ['토', '구토'],\n",
        "        'blood': ['피'],\n",
        "        'heat_rash': ['땀띠'],\n",
        "        'seborrheic': ['태열'],\n",
        "        'flu': ['독감'],\n",
        "        'pneumonia': ['폐렴'],\n",
        "        'phlegm': ['가래'],\n",
        "        'rash': ['발진'],\n",
        "        'blister': ['수포'],\n",
        "        'otitis': ['중이염']\n",
        "    }\n",
        "\n",
        "    display_name_mapping = {\n",
        "        'hospital': '병원/소아과',\n",
        "        'symptom': '증상',\n",
        "        'skin': '피부',\n",
        "        'fever': '열',\n",
        "        'cough': '기침',\n",
        "        'runny_nose': '콧물',\n",
        "        'stool': '변/똥',\n",
        "        'hand_foot': '수족',\n",
        "        'diarrhea': '설사',\n",
        "        'allergy': '알러지/알레르기',\n",
        "        'atopy': '아토피',\n",
        "        'urticaria': '두드러기',\n",
        "        'vomit': '토/구토',\n",
        "        'blood': '피',\n",
        "        'heat_rash': '땀띠',\n",
        "        'seborrheic': '태열',\n",
        "        'flu': '독감',\n",
        "        'pneumonia': '폐렴',\n",
        "        'phlegm': '가래',\n",
        "        'rash': '발진',\n",
        "        'blister': '수포',\n",
        "        'otitis': '중이염'\n",
        "    }\n",
        "\n",
        "    # 키워드 그룹 차원 테이블 생성\n",
        "    keyword_groups_df = pd.DataFrame([\n",
        "        {'group_id': i+1, 'group_name': group_name, 'display_name': display_name_mapping.get(group_name, group_name), 'description': ''}\n",
        "        for i, group_name in enumerate(keyword_groups.keys())\n",
        "    ])\n",
        "\n",
        "    # 키워드 사전 테이블 생성\n",
        "    keywords_list = []\n",
        "    for group_id, (group_name, keywords) in enumerate(keyword_groups.items(), 1):\n",
        "        for keyword in keywords:\n",
        "            keywords_list.append({\n",
        "                'keyword': keyword,\n",
        "                'group_id': group_id,\n",
        "                'is_stopword': keyword in FILTER_WORDS\n",
        "            })\n",
        "    keywords_df = pd.DataFrame(keywords_list)\n",
        "\n",
        "    # 게시글-키워드 그룹 연결 테이블 생성\n",
        "    def has_keywords(text, keywords):\n",
        "        return any(keyword in text for keyword in keywords)\n",
        "\n",
        "    post_keyword_groups_list = []\n",
        "    for post_id in df['post_id']:\n",
        "        post_text = df[df['post_id'] == post_id]['question_text'].iloc[0]\n",
        "        for group_id, (group_name, keywords) in enumerate(keyword_groups.items(), 1):\n",
        "            has_keyword = has_keywords(post_text, keywords)\n",
        "            post_keyword_groups_list.append({\n",
        "                'post_id': post_id,\n",
        "                'group_id': group_id,\n",
        "                'has_keyword': has_keyword\n",
        "            })\n",
        "    post_keyword_groups_df = pd.DataFrame(post_keyword_groups_list)\n",
        "\n",
        "    # 6. 토큰 분석 테이블 생성\n",
        "    print(\"토큰 분석 테이블 생성 중...\")\n",
        "    tokens_fact_list = []\n",
        "    token_id = 1\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        post_id = row['post_id']\n",
        "        token_position = 0\n",
        "\n",
        "        # 각 토큰 정보 처리\n",
        "        token_counts = {}  # 같은 토큰이 여러 번 나오는 경우를 위한 카운팅\n",
        "\n",
        "        for token_info in row['tokens_with_pos']:\n",
        "            token = token_info['token']\n",
        "            pos = token_info['pos']\n",
        "            is_stopword = token_info['is_stopword']\n",
        "\n",
        "            # 토큰 카운트 증가\n",
        "            if token in token_counts:\n",
        "                token_counts[token] += 1\n",
        "            else:\n",
        "                token_counts[token] = 1\n",
        "\n",
        "            tokens_fact_list.append({\n",
        "                'token_id': token_id,\n",
        "                'post_id': post_id,\n",
        "                'token': token,\n",
        "                'token_pos': pos,\n",
        "                'token_count': token_counts[token],\n",
        "                'token_position': token_position,\n",
        "                'is_stopword': is_stopword\n",
        "            })\n",
        "\n",
        "            token_id += 1\n",
        "            token_position += 1\n",
        "\n",
        "    tokens_fact_df = pd.DataFrame(tokens_fact_list)\n",
        "\n",
        "    # 7. 게시글 기본 정보 테이블 준비\n",
        "    print(\"게시글 기본 정보 테이블 준비 중...\")\n",
        "    posts_dim_df = df[['post_id', 'url', 'raw_date', 'post_date', 'post_month', 'post_year', 'author', 'question_text']]\n",
        "\n",
        "    # 8. 출력 디렉토리 생성\n",
        "    print(\"로컬에 데이터 저장 중...\")\n",
        "    os.makedirs(os.path.join(output_path, 'babycareai/posts_dim'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_path, 'babycareai/tokens_fact'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_path, 'babycareai/keyword_groups_dim'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_path, 'babycareai/keywords_dictionary'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_path, 'babycareai/post_keyword_groups'), exist_ok=True)\n",
        "\n",
        "    # posts_dim 저장\n",
        "    save_df_to_parquet(posts_dim_df, os.path.join(output_path, 'babycareai/posts_dim/posts_dim.parquet'))\n",
        "\n",
        "    # tokens_fact 저장 (큰 데이터는 청크로 나누어 저장)\n",
        "    chunk_size = 100000  # 조정 가능\n",
        "    for i, chunk_df in enumerate(chunk_generator(tokens_fact_df, chunk_size)):\n",
        "        save_df_to_parquet(chunk_df, os.path.join(output_path, f'babycareai/tokens_fact/tokens_fact_part_{i:03d}.parquet'))\n",
        "\n",
        "    # keyword_groups_dim 저장\n",
        "    save_df_to_parquet(keyword_groups_df, os.path.join(output_path, 'babycareai/keyword_groups_dim/keyword_groups_dim.parquet'))\n",
        "\n",
        "    # keywords_dictionary 저장\n",
        "    save_df_to_parquet(keywords_df, os.path.join(output_path, 'babycareai/keywords_dictionary/keywords_dictionary.parquet'))\n",
        "\n",
        "    # post_keyword_groups 저장 (큰 데이터는 청크로 나누어 저장)\n",
        "    for i, chunk_df in enumerate(chunk_generator(post_keyword_groups_df, chunk_size)):\n",
        "        save_df_to_parquet(chunk_df, os.path.join(output_path, f'babycareai/post_keyword_groups/post_keyword_groups_part_{i:03d}.parquet'))\n",
        "\n",
        "    print(\"데이터 처리 및 저장 완료!\")"
      ],
      "metadata": {
        "id": "lFn4LBe1ntsZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_generator(df, chunk_size):\n",
        "    \"\"\"데이터프레임을 청크로 나누어 생성하는 제너레이터\"\"\"\n",
        "    for i in range(0, len(df), chunk_size):\n",
        "        yield df.iloc[i:i + chunk_size]"
      ],
      "metadata": {
        "id": "fdkYdQc0ntpy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_df_to_parquet(df, file_path):\n",
        "    \"\"\"DataFrame을 Parquet 형식으로 로컬에 저장\"\"\"\n",
        "    # DataFrame을 Parquet으로 변환\n",
        "    table = pa.Table.from_pandas(df)\n",
        "    pq.write_table(table, file_path)\n",
        "    print(f\"'{file_path}'에 파일 저장 완료\")"
      ],
      "metadata": {
        "id": "V942fRrLntnW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용 예시\n",
        "if __name__ == \"__main__\":\n",
        "    # 실행 파라미터\n",
        "    input_parquet_path = '/content/drive/MyDrive/babycareai/part-00000-0d16091a-7fa1-464f-9f1c-b2c0bf27fe0a-c000.snappy.parquet'\n",
        "    output_path = '/content/drive/MyDrive/babycareai'\n",
        "    stopwords_path = '/content/drive/MyDrive/babycareai/stopwords.txt'\n",
        "\n",
        "    # 처리 실행\n",
        "    process_data_for_local(input_parquet_path, output_path, stopwords_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqL_7pjqntkr",
        "outputId": "abb846d6-b918-409f-9338-8fcbd79215f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 로드 중...\n",
            "날짜 변환 중...\n",
            "토큰 추출 중...\n",
            "각 게시글에서 토큰 추출 중...\n",
            "키워드 그룹 정보 추출 중...\n",
            "토큰 분석 테이블 생성 중...\n",
            "게시글 기본 정보 테이블 준비 중...\n",
            "로컬에 데이터 저장 중...\n",
            "'/content/drive/MyDrive/babycareai/babycareai/posts_dim/posts_dim.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_000.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_001.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_002.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_003.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_004.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_005.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_006.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_007.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_008.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_009.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_010.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_011.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_012.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/tokens_fact/tokens_fact_part_013.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/keyword_groups_dim/keyword_groups_dim.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/keywords_dictionary/keywords_dictionary.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_000.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_001.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_002.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_003.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_004.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_005.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_006.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_007.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_008.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_009.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_010.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_011.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_012.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_013.parquet'에 파일 저장 완료\n",
            "'/content/drive/MyDrive/babycareai/babycareai/post_keyword_groups/post_keyword_groups_part_014.parquet'에 파일 저장 완료\n",
            "데이터 처리 및 저장 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HdMMd3_GntiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wpo8D7y5ntfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bsp9K0YJntdT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}